{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_vader(sentiment):\n",
    "    if sentiment['compound'] > 0.05:\n",
    "        return 1\n",
    "    elif sentiment['compound'] < -0.05:\n",
    "        return -1\n",
    "    else:\n",
    "       return 0\n",
    "\n",
    "def categorize_textblob(sentiment):\n",
    "    if sentiment> 0.05:\n",
    "        return 1\n",
    "    elif sentiment < -0.05:\n",
    "        return -1\n",
    "    else:\n",
    "       return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = pd.read_csv(\"manual_labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = labeled_tweets[labeled_tweets[\"label\"] != np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = labeled_tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets = labeled_tweets[(labeled_tweets[\"label\"] == \"1\") | (labeled_tweets[\"label\"] == \"0\") | (labeled_tweets[\"label\"] == \"-1\" )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nathan\\AppData\\Local\\Temp/ipykernel_1032/422618980.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_tweets[\"label\"] = good_tweets[\"label\"].apply(int)\n"
     ]
    }
   ],
   "source": [
    "good_tweets[\"label\"] = good_tweets[\"label\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets = good_tweets.set_index(\"tweet_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run vader to find sentiment on tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets[\"vader\"] = good_tweets[\"tweet\"].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets[\"vader_category\"] = good_tweets[\"vader\"].apply(categorize_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    704\n",
       "-1    488\n",
       " 0    397\n",
       "Name: vader_category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_tweets[\"vader_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.56      0.60       556\n",
      "           0       0.37      0.41      0.39       353\n",
      "           1       0.59      0.61      0.60       680\n",
      "\n",
      "    accuracy                           0.55      1589\n",
      "   macro avg       0.53      0.53      0.53      1589\n",
      "weighted avg       0.56      0.55      0.55      1589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(good_tweets[\"label\"], good_tweets[\"vader_category\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run textblob to find sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets[\"textblob\"] = good_tweets[\"tweet\"].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets[\"textblob_category\"] = good_tweets[\"textblob\"].apply(categorize_textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    654\n",
       " 0    640\n",
       "-1    295\n",
       "Name: textblob_category, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_tweets[\"textblob_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.28      0.37       556\n",
      "           0       0.24      0.44      0.31       353\n",
      "           1       0.53      0.51      0.52       680\n",
      "\n",
      "    accuracy                           0.41      1589\n",
      "   macro avg       0.43      0.41      0.40      1589\n",
      "weighted avg       0.47      0.41      0.42      1589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(good_tweets[\"label\"], good_tweets[\"textblob_category\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words vectorization naieve bayes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.64      0.69       122\n",
      "           0       0.57      0.44      0.50        86\n",
      "           1       0.70      0.83      0.76       190\n",
      "\n",
      "    accuracy                           0.69       398\n",
      "   macro avg       0.67      0.64      0.65       398\n",
      "weighted avg       0.68      0.69      0.68       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\n",
    "#Loading the Dataset\n",
    "data = good_tweets\n",
    "\n",
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "#cv = TfidfVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "\n",
    "text_counts = cv.fit_transform(data['tweet'])\n",
    "#Splitting the data into trainig and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['label'], test_size=0.25, random_state=random.randint(0,100000))\n",
    "#Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "#Caluclating the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print(classification_report(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.64      0.69       122\n",
      "           0       0.57      0.44      0.50        86\n",
      "           1       0.70      0.83      0.76       190\n",
      "\n",
      "    accuracy                           0.69       398\n",
      "   macro avg       0.67      0.64      0.65       398\n",
      "weighted avg       0.68      0.69      0.68       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets[\"bayes_category\"] = MNB.predict(cv.transform(good_tweets[\"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tweets.to_csv(\"tweets_with_label_and_pretrained.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#from https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\\n#Importing necessary libraries\\nimport nltk\\nimport pandas as pd\\nfrom textblob import Word\\nfrom nltk.corpus import stopwords\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\\nfrom keras.models import Sequential\\nfrom keras.preprocessing.text import Tokenizer\\nfrom keras_preprocessing.sequence import pad_sequences\\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\\nfrom sklearn.model_selection import train_test_split \\n#Loading the dataset\\ndata = good_tweets\\n#Pre-Processing the text \\ndef cleaning(df, stop_words):\\n    df[\\'tweet\\'] = df[\\'tweet\\'].apply(lambda x: \\' \\'.join(x.lower() for x in x.split()))\\n    # Replacing the digits/numbers\\n    df[\\'tweet\\'] = df[\\'tweet\\'].str.replace(\\'d\\', \\'\\')\\n    # Removing stop words\\n    df[\\'tweet\\'] = df[\\'tweet\\'].apply(lambda x: \\' \\'.join(x for x in x.split() if x not in stop_words))\\n    # Lemmatization\\n    df[\\'tweet\\'] = df[\\'tweet\\'].apply(lambda x: \\' \\'.join([Word(x).lemmatize() for x in x.split()]))\\n    return df\\nstop_words = stopwords.words(\\'english\\')\\ndata_cleaned = cleaning(data, stop_words)\\n#Generating Embeddings using tokenizer\\ntokenizer = Tokenizer(num_words=500, split=\\' \\') \\ntokenizer.fit_on_texts(data_cleaned[\\'tweet\\'].values)\\nX = tokenizer.texts_to_sequences(data_cleaned[\\'tweet\\'].values)\\nX = pad_sequences(X)\\n#Model Building\\nmodel = Sequential()\\nmodel.add(Embedding(700, 120, input_length = X.shape[1]))\\nmodel.add(SpatialDropout1D(0.4))\\nmodel.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\\nmodel.add(Dense(352, activation=\\'LeakyReLU\\'))\\nmodel.add(Dense(3, activation=\\'softmax\\'))\\nmodel.compile(loss = \\'sparse_categorical_crossentropy\\', optimizer=\\'adam\\', metrics = [\\'accuracy\\'])\\nprint(model.summary())\\n#Model Training\\nimport random\\nX_train1, X_test1, Y_train1, Y_test1 = train_test_split(X, data[\\'label\\']+np.ones(data[\"label\"].shape), test_size=0.25, random_state=random.randint(0,100000))\\nmodel.fit(X_train1, Y_train1, epochs = 10, batch_size=32, verbose =1)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#from https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\n",
    "#Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Loading the dataset\n",
    "data = good_tweets\n",
    "#Pre-Processing the text \n",
    "def cleaning(df, stop_words):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "    # Replacing the digits/numbers\n",
    "    df['tweet'] = df['tweet'].str.replace('d', '')\n",
    "    # Removing stop words\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
    "    # Lemmatization\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "    return df\n",
    "stop_words = stopwords.words('english')\n",
    "data_cleaned = cleaning(data, stop_words)\n",
    "#Generating Embeddings using tokenizer\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(data_cleaned['tweet'].values)\n",
    "X = tokenizer.texts_to_sequences(data_cleaned['tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "#Model Building\n",
    "model = Sequential()\n",
    "model.add(Embedding(700, 120, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(352, activation='LeakyReLU'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#Model Training\n",
    "import random\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X, data['label']+np.ones(data[\"label\"].shape), test_size=0.25, random_state=random.randint(0,100000))\n",
    "model.fit(X_train1, Y_train1, epochs = 10, batch_size=32, verbose =1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Model Testing\\nmodel.evaluate(X_test1,Y_test1)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#Model Testing\n",
    "model.evaluate(X_test1,Y_test1)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
