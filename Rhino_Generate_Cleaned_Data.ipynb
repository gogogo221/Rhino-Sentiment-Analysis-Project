{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SY-O0Lkn4EHG"
      },
      "outputs": [],
      "source": [
        "from numpy import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import io\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuj_Yh5bPtA_",
        "outputId": "a81a0106-1861-4f0e-c395-e2c8e5bcdfe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\nathan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\nathan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\nathan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\nathan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "TWEETS_PATH = r'D:/coding/classes/csci461/Rhino-Sentiment-Analysis-Project/filtered_tweets'\n",
        "all_files = glob.glob(os.path.join(TWEETS_PATH , \"*.csv\"))\n",
        "OUTPUT_PATH = r\"D:\\coding\\classes\\csci461\\Rhino-Sentiment-Analysis-Project\\processed_tweets\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FaDu89MF5KdA"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# This cell contains functions to read and cleanup a dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "def read_and_cleanup_dataset(filename):\n",
        "  # Read dataset into dataframe\n",
        "  tweets = pd.read_csv(filename)\n",
        "\n",
        "  # Drop none English tweets, comment out if pre-processed\n",
        "  tweets = tweets[tweets.lang == \"en\"]\n",
        "  tweets.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Define patterns to be excluded, currently the following:\n",
        "  # special characters, @someone, &sth, rt, new line, link, any extra space\n",
        "  pattern = r'[!@#\\$%\\^&\\*\\(\\)\\[\\]{};:\\'\",.<>/?\\\\|_~`-]+|@\\w+|&\\w+|rt|\\n|rhino|https://\\S+'\n",
        "\n",
        "  # pattern for white space\n",
        "  ws = r'\\s+'\n",
        "\n",
        "  # non-ASCII specific quotes\n",
        "  quotes_to_remove = ['“', '”', '‘', '‛', '’']\n",
        "\n",
        "  # Define emoji patterns to be excluded\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # Emojis in the first group\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # Emojis in the second group\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # Emojis in the third group\n",
        "                           u\"\\U0001F700-\\U0001F77F\"  # Emojis in the fourth group\n",
        "                           u\"\\U0001F780-\\U0001F7FF\"  # Emojis in the fifth group\n",
        "                           u\"\\U0001F800-\\U0001F8FF\"  # Emojis in the sixth group\n",
        "                           u\"\\U0001F900-\\U0001F9FF\"  # Emojis in the seventh group\n",
        "                           u\"\\U0001FA00-\\U0001FA6F\"  # Emojis in the eighth group\n",
        "                           u\"\\U0001FA70-\\U0001FAFF\"  # Emojis in the ninth group\n",
        "                           u\"\\U0001F200-\\U0001F251\"  # Emojis in the tenth group\n",
        "                           u\"\\U0001F004-\\U0001F0CF\"  # Additional emojis\n",
        "                           u\"\\U0001F10D-\\U0001F10F\"  # Additional emojis\n",
        "                           u\"\\U0001F30D-\\U0001F567\"  # Additional emojis\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "  # Perform lower casing and remove any patterns\n",
        "  tweets['tweet'] = tweets['tweet'].apply(str.lower)\n",
        "  tweets['tweet'] = tweets['tweet'].str.replace(pattern, '', regex=True)\n",
        "  tweets['tweet'] = tweets['tweet'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
        "  tweets['tweet'] = tweets['tweet'].str.replace(ws, ' ', regex=True).str.strip()\n",
        "  for quote in quotes_to_remove:\n",
        "    tweets['tweet'] = tweets['tweet'].str.replace(quote, '')\n",
        "\n",
        "  return tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vPxFhwewLcNP"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# This cell contains functions to perform tokenization and remove stop words\n",
        "# -----------------------------------------------------------------------------\n",
        "# Constants\n",
        "# POS (Parts Of Speech) for: nouns, adjectives, verbs and adverbs\n",
        "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n",
        "POS_TYPES = list(DI_POS_TYPES.keys())\n",
        "\n",
        "# Constraints on tokens\n",
        "MIN_STR_LEN = 3\n",
        "RE_VALID = '[a-zA-Z]'\n",
        "\n",
        "# Remove accents function\n",
        "def remove_accents(data):\n",
        "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "def Remove_Stop_Words(li_tweets):\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "  # Process all quotes\n",
        "  li_tokens = []\n",
        "  li_token_lists = []\n",
        "  li_lem_strings = []\n",
        "\n",
        "  for i,text in enumerate(li_tweets):\n",
        "      # Tokenize by sentence, then by lowercase word\n",
        "      tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "\n",
        "      # Process all tokens per quote\n",
        "      li_tokens_quote = []\n",
        "      li_tokens_quote_lem = []\n",
        "      for token in tokens:\n",
        "          # Remove accents\n",
        "          t = remove_accents(token)\n",
        "\n",
        "          # Remove punctuation\n",
        "          t = str(t).translate(string.punctuation)\n",
        "          li_tokens_quote.append(t)\n",
        "\n",
        "          # Add token that represents \"no lemmatization match\"\n",
        "          li_tokens_quote_lem.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
        "\n",
        "          # Process each token\n",
        "          if t not in stopwords:\n",
        "              if re.search(RE_VALID, t):\n",
        "                  if len(t) >= MIN_STR_LEN:\n",
        "                      # Note that the POS (Part Of Speech) is necessary as input to the lemmatizer\n",
        "                      # (otherwise it assumes the word is a noun)\n",
        "                      pos = nltk.pos_tag([t])[0][1][:2]\n",
        "                      pos2 = 'n'  # set default to noun\n",
        "                      if pos in DI_POS_TYPES:\n",
        "                        pos2 = DI_POS_TYPES[pos]\n",
        "\n",
        "                      stem = stemmer.stem(t)\n",
        "                      lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
        "\n",
        "                      if pos in POS_TYPES:\n",
        "                          li_tokens.append((t, stem, lem, pos))\n",
        "\n",
        "                          # Remove the \"-\" token and append the lemmatization match\n",
        "                          li_tokens_quote_lem = li_tokens_quote_lem[:-1]\n",
        "                          li_tokens_quote_lem.append(lem)\n",
        "\n",
        "      # Build list of token lists from lemmatized tokens\n",
        "      li_token_lists.append(li_tokens_quote)\n",
        "\n",
        "      # Build list of strings from lemmatized tokens\n",
        "      str_li_tokens_quote_lem = ' '.join(li_tokens_quote_lem)\n",
        "      cleaned_str_li_tokens_quote_lem = str_li_tokens_quote_lem.replace('-', '')\n",
        "      li_lem_strings.append(cleaned_str_li_tokens_quote_lem)\n",
        "  return li_lem_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RhoqD3exOQim"
      },
      "outputs": [],
      "source": [
        "for file_path in all_files:\n",
        "    tweets = read_and_cleanup_dataset(file_path)\n",
        "    li_tweets = tweets['tweet'].tolist()\n",
        "    li_lem_strings = Remove_Stop_Words(li_tweets)\n",
        "    tweets['tweet'] = li_lem_strings\n",
        "    tweets['tweet'] = tweets['tweet'].str.replace('\\s+', ' ', regex=True)\n",
        "    tweets = tweets.drop_duplicates(subset=\"tweet\", keep=\"first\",ignore_index=True)\n",
        "    filename = file_path.split(\"\\\\\")[-1]\n",
        "    tweets.to_csv(OUTPUT_PATH + \"/\" + filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFN1AumxQs8S",
        "outputId": "f48f0674-93fa-4b00-ec4d-1c1a3cf10478"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FzcYCU85YjF"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
